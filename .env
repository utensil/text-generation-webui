# by default the Dockerfile specifies these versions: 3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX
# however for me to work i had to specify the exact version for my card ( 2060 ) it was 7.5
# https://developer.nvidia.com/cuda-gpus you can find the version for your card here
TORCH_CUDA_ARCH_LIST=3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX

# WORKS and FAST
# Loaded the model in 32.15 seconds.
# Output generated in 6.73 seconds (9.36 tokens/s, 63 tokens, context 62)
# CLI_ARGS=--model alpaca-native-4bit --model_type llama --wbits 4 --groupsize 128 --listen --auto-devices --chat

# LOAD and FAILD at inference
# Loaded the model in 146.68 seconds.
# RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
# CLI_ARGS=--model alpaca-native  --model_type llama --listen --auto-devices --chat

# WORKS and NORMAL
# Loaded the model in 67.07 seconds.
# Output generated in 78.61 seconds (0.92 tokens/s, 72 tokens, context 63)
# CLI_ARGS=--model alpaca-native  --model_type llama  --load-in-8bit --listen --auto-devices --gpu-memory 5 --chat

# Loaded the model in 111.99 seconds.
# RuntimeError: expected scalar type Float but found Half
# CLI_ARGS=--model TheBloke_galpaca-30B-GPTQ-4bit-128g --wbits 4 --groupsize 128 --listen --auto-devices --gpu-memory 5 --chat

# Loaded the model in 100.13 seconds.
# RuntimeError: "LayerNormKernelImpl" not implemented for 'Half'
# CLI_ARGS=--model TheBloke_galpaca-30B-GPTQ-4bit-128g --wbits 4 --groupsize 128 --listen --cpu --chat

CLI_ARGS=--model TheBloke_galpaca-30B-GPTQ-4bit-128g --wbits 4 --groupsize 128 --listen --chat

# the following examples have been tested with the files linked in docs/README_docker.md:
# example running 13b with 4bit/128 groupsize        : CLI_ARGS=--model llama-13b-4bit-128g --wbits 4 --listen --groupsize 128 --pre_layer 25
# example with loading api extension and public share: CLI_ARGS=--model llama-7b-4bit --wbits 4 --listen --auto-devices --no-stream --extensions api --share
# example running 7b with 8bit groupsize             : CLI_ARGS=--model llama-7b --load-in-8bit --listen --auto-devices

# the port the webui binds to on the host
HOST_PORT=7860
# the port the webui binds to inside the container
CONTAINER_PORT=7860

# the port the api binds to on the host
HOST_API_PORT=5000
# the port the api binds to inside the container
CONTAINER_API_PORT=5000

# the version used to install text-generation-webui from
WEBUI_VERSION=HEAD
